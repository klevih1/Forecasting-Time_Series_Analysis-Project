{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f8ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE PM2.5 FORECASTING PIPELINE\n",
      "9 Models: 3 Statistical | 3 Machine Learning | 3 Deep Learning\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "City: Beijing\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "ARIMA                Statistical         32.19 ± 12.73   39.24      63.2%\n",
      "SARIMA               Statistical         32.62 ± 12.75   40.05      61.3%\n",
      "Exp Smoothing        Statistical         33.39 ± 9.80   41.13      60.6%\n",
      "Gradient Boosting    Machine Learning    27.02 ± 10.84   33.94      58.7% ⭐\n",
      "Random Forest        Machine Learning    27.61 ± 10.51   34.23      62.7%\n",
      "SVR                  Machine Learning    29.35 ± 11.52   35.46      64.6%\n",
      "GRU                  Deep Learning       31.79 ± 11.78   38.93      69.4%\n",
      "CNN-LSTM             Deep Learning       32.11 ± 12.96   38.71      64.9%\n",
      "LSTM                 Deep Learning       32.12 ± 12.06   39.06      67.9%\n",
      "\n",
      "Best Model: Gradient Boosting (MAE: 27.02)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "City: Berlin\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "SARIMA               Statistical         12.25 ± 4.27   15.20      30.2%\n",
      "ARIMA                Statistical         12.30 ± 3.95   15.21      30.9%\n",
      "Exp Smoothing        Statistical         18.47 ± 9.47   21.52      51.4%\n",
      "Random Forest        Machine Learning     8.72 ± 2.15   10.90      24.1% ⭐\n",
      "Gradient Boosting    Machine Learning     8.74 ± 1.98   11.24      24.7%\n",
      "SVR                  Machine Learning     9.14 ± 2.01   11.55      25.9%\n",
      "GRU                  Deep Learning       12.49 ± 2.42   15.33      34.5%\n",
      "CNN-LSTM             Deep Learning       13.26 ± 2.39   16.07      38.3%\n",
      "LSTM                 Deep Learning       13.66 ± 4.09   16.55      36.1%\n",
      "\n",
      "Best Model: Random Forest (MAE: 8.72)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "City: Boston\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "SARIMA               Statistical          9.62 ± 2.99   11.90      54.5%\n",
      "ARIMA                Statistical          9.77 ± 2.83   12.01      56.4%\n",
      "Exp Smoothing        Statistical         13.59 ± 10.40   16.32      70.9%\n",
      "Random Forest        Machine Learning     8.20 ± 2.73   10.65      48.4% ⭐\n",
      "Gradient Boosting    Machine Learning     8.52 ± 3.11   11.05      49.1%\n",
      "SVR                  Machine Learning     8.73 ± 4.05   11.16      48.6%\n",
      "CNN-LSTM             Deep Learning        9.85 ± 3.81   12.09      58.1%\n",
      "GRU                  Deep Learning       10.22 ± 3.02   12.59      59.9%\n",
      "LSTM                 Deep Learning       10.35 ± 3.03   12.49      63.4%\n",
      "\n",
      "Best Model: Random Forest (MAE: 8.20)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "City: London\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "SARIMA               Statistical         10.01 ± 4.08   11.96      37.4%\n",
      "ARIMA                Statistical         10.08 ± 4.08   11.99      38.1%\n",
      "Exp Smoothing        Statistical         13.41 ± 2.58   15.92      46.6%\n",
      "Random Forest        Machine Learning     8.20 ± 2.24   10.25      32.4% ⭐\n",
      "Gradient Boosting    Machine Learning     9.24 ± 2.72   11.22      35.9%\n",
      "SVR                  Machine Learning     9.90 ± 3.03   12.09      37.7%\n",
      "GRU                  Deep Learning        9.09 ± 4.26   10.75      33.2%\n",
      "LSTM                 Deep Learning        9.72 ± 5.53   11.40      38.5%\n",
      "CNN-LSTM             Deep Learning       10.51 ± 6.21   12.39      42.4%\n",
      "\n",
      "Best Model: Random Forest (MAE: 8.20)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "City: Paris\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "SARIMA               Statistical         10.77 ± 1.48   13.40      31.9%\n",
      "ARIMA                Statistical         11.01 ± 1.51   13.51      33.2%\n",
      "Exp Smoothing        Statistical         16.20 ± 6.03   20.00      46.2%\n",
      "Random Forest        Machine Learning     8.35 ± 1.53   10.66      24.9% ⭐\n",
      "SVR                  Machine Learning     8.73 ± 1.74   11.01      25.4%\n",
      "Gradient Boosting    Machine Learning     8.76 ± 1.33   11.14      26.4%\n",
      "CNN-LSTM             Deep Learning       10.84 ± 1.39   13.21      34.0%\n",
      "LSTM                 Deep Learning       11.12 ± 1.66   13.49      34.5%\n",
      "GRU                  Deep Learning       11.13 ± 1.49   13.51      36.1%\n",
      "\n",
      "Best Model: Random Forest (MAE: 8.35)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "City: Tokyo\n",
      "================================================================================\n",
      "Running 5-Fold Time Series Cross-Validation...\n",
      "  Fold 1/5... ✓\n",
      "  Fold 2/5... ✓\n",
      "  Fold 3/5... ✓\n",
      "  Fold 4/5... ✓\n",
      "  Fold 5/5... ✓\n",
      "\n",
      "Model                Family             MAE          RMSE         MAPE\n",
      "--------------------------------------------------------------------------------\n",
      "ARIMA                Statistical         13.18 ± 2.26   15.50      44.5%\n",
      "SARIMA               Statistical         13.27 ± 2.31   15.63      44.4%\n",
      "Exp Smoothing        Statistical         19.90 ± 7.05   23.19      63.2%\n",
      "Random Forest        Machine Learning    10.45 ± 3.14   12.90      36.6% ⭐\n",
      "Gradient Boosting    Machine Learning    10.59 ± 2.33   12.85      35.8%\n",
      "SVR                  Machine Learning    13.27 ± 2.69   16.14      42.5%\n",
      "GRU                  Deep Learning       12.88 ± 3.31   15.63      38.9%\n",
      "CNN-LSTM             Deep Learning       13.45 ± 3.94   16.27      41.7%\n",
      "LSTM                 Deep Learning       14.62 ± 3.60   17.46      48.2%\n",
      "\n",
      "Best Model: Random Forest (MAE: 10.45)\n",
      "Retraining on full dataset...\n",
      "\n",
      "================================================================================\n",
      "FORECASTING COMPLETE!\n",
      "================================================================================\n",
      "✓ Predictions saved: c:\\Users\\klevi\\Desktop\\Time Series Analysis\\Air_Quality_Index-CAPSTONE_PROJECT\\predictions\n",
      "✓ Model comparison saved: c:\\Users\\klevi\\Desktop\\Time Series Analysis\\Air_Quality_Index-CAPSTONE_PROJECT\\outputs/model_comparison.json\n",
      "\n",
      "Best Models by City:\n",
      "  Beijing         → Gradient Boosting    (MAE: 27.02)\n",
      "  Berlin          → Random Forest        (MAE: 8.72)\n",
      "  Boston          → Random Forest        (MAE: 8.20)\n",
      "  London          → Random Forest        (MAE: 8.20)\n",
      "  Paris           → Random Forest        (MAE: 8.35)\n",
      "  Tokyo           → Random Forest        (MAE: 10.45)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    KERAS_AVAILABLE = True\n",
    "except:\n",
    "    KERAS_AVAILABLE = False\n",
    "    print(\"Warning: TensorFlow not available. Deep learning models will be skipped.\")\n",
    "\n",
    "# Statistical Models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd()) \n",
    "FORECAST_FILE = os.path.join(BASE_DIR, \"data\", \"processed\", \"forecast_ready.parquet\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "PREDICTIONS_DIR = os.path.join(BASE_DIR, \"predictions\")\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Forecast horizon\n",
    "FORECAST_START = '2026-01-01'\n",
    "FORECAST_END = '2026-01-31'\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE PM2.5 FORECASTING PIPELINE\")\n",
    "print(\"9 Models: 3 Statistical | 3 Machine Learning | 3 Deep Learning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_parquet(FORECAST_FILE)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING ---\n",
    "def create_features(data, city_name):\n",
    "    \"\"\"Create time-based and lag features\"\"\"\n",
    "    df_city = data[data['City'] == city_name].copy()\n",
    "    df_city = df_city.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Time features\n",
    "    df_city['year'] = df_city['Date'].dt.year\n",
    "    df_city['month'] = df_city['Date'].dt.month\n",
    "    df_city['day'] = df_city['Date'].dt.day\n",
    "    df_city['dayofweek'] = df_city['Date'].dt.dayofweek\n",
    "    df_city['dayofyear'] = df_city['Date'].dt.dayofyear\n",
    "    df_city['quarter'] = df_city['Date'].dt.quarter\n",
    "    df_city['weekofyear'] = df_city['Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df_city['month_sin'] = np.sin(2 * np.pi * df_city['month'] / 12)\n",
    "    df_city['month_cos'] = np.cos(2 * np.pi * df_city['month'] / 12)\n",
    "    df_city['day_sin'] = np.sin(2 * np.pi * df_city['dayofweek'] / 7)\n",
    "    df_city['day_cos'] = np.cos(2 * np.pi * df_city['dayofweek'] / 7)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:\n",
    "        df_city[f'lag_{lag}'] = df_city['median'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [7, 14, 30]:\n",
    "        df_city[f'rolling_mean_{window}'] = df_city['median'].shift(1).rolling(window).mean()\n",
    "        df_city[f'rolling_std_{window}'] = df_city['median'].shift(1).rolling(window).std()\n",
    "        df_city[f'rolling_min_{window}'] = df_city['median'].shift(1).rolling(window).min()\n",
    "        df_city[f'rolling_max_{window}'] = df_city['median'].shift(1).rolling(window).max()\n",
    "    \n",
    "    # EWMA\n",
    "    df_city['ewm_7'] = df_city['median'].shift(1).ewm(span=7).mean()\n",
    "    df_city['ewm_30'] = df_city['median'].shift(1).ewm(span=30).mean()\n",
    "    \n",
    "    return df_city\n",
    "\n",
    "# --- 3. TIME SERIES CV ---\n",
    "def time_series_cv_split(data, n_splits=5, test_size=30):\n",
    "    \"\"\"Create time series cross-validation splits\"\"\"\n",
    "    splits = []\n",
    "    total_size = len(data)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        test_end = total_size - i * test_size\n",
    "        test_start = test_end - test_size\n",
    "        train_end = test_start\n",
    "        \n",
    "        if train_end < 100:\n",
    "            break\n",
    "            \n",
    "        train_idx = data.index[:train_end]\n",
    "        test_idx = data.index[test_start:test_end]\n",
    "        splits.append((train_idx, test_idx))\n",
    "    \n",
    "    return splits[::-1]\n",
    "\n",
    "# --- 4. MODEL DEFINITIONS ---\n",
    "\n",
    "# STATISTICAL MODELS\n",
    "def train_arima(y_train, y_test):\n",
    "    \"\"\"ARIMA - AutoRegressive Integrated Moving Average\"\"\"\n",
    "    try:\n",
    "        model = ARIMA(y_train, order=(5, 1, 2))\n",
    "        fitted = model.fit()\n",
    "        forecast = fitted.forecast(steps=len(y_test))\n",
    "        return forecast.values\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean())\n",
    "\n",
    "def train_sarima(y_train, y_test):\n",
    "    \"\"\"SARIMA - Seasonal ARIMA\"\"\"\n",
    "    try:\n",
    "        model = SARIMAX(y_train, order=(2, 1, 2), seasonal_order=(1, 1, 1, 7))\n",
    "        fitted = model.fit(disp=False)\n",
    "        forecast = fitted.forecast(steps=len(y_test))\n",
    "        return forecast.values\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean())\n",
    "\n",
    "def train_exponential_smoothing(y_train, y_test):\n",
    "    \"\"\"Exponential Smoothing (Holt-Winters)\"\"\"\n",
    "    try:\n",
    "        model = ExponentialSmoothing(y_train, seasonal_periods=7, trend='add', seasonal='add')\n",
    "        fitted = model.fit()\n",
    "        forecast = fitted.forecast(steps=len(y_test))\n",
    "        return forecast.values\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean())\n",
    "\n",
    "# MACHINE LEARNING MODELS\n",
    "def train_random_forest(X_train, y_train, X_test):\n",
    "    \"\"\"Random Forest Regressor\"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test), model\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train, X_test):\n",
    "    \"\"\"Gradient Boosting Regressor\"\"\"\n",
    "    model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test), model\n",
    "\n",
    "def train_svr(X_train, y_train, X_test):\n",
    "    \"\"\"Support Vector Regression\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    return model.predict(X_test_scaled), (model, scaler)\n",
    "\n",
    "# DEEP LEARNING MODELS\n",
    "def create_sequences(data, lookback=30):\n",
    "    \"\"\"Create sequences for deep learning\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i-lookback:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_lstm(y_train, y_test, lookback=30):\n",
    "    \"\"\"LSTM - Long Short-Term Memory Network\"\"\"\n",
    "    if not KERAS_AVAILABLE:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "    \n",
    "    try:\n",
    "        # Normalize\n",
    "        train_mean, train_std = y_train.mean(), y_train.std()\n",
    "        y_train_norm = (y_train - train_mean) / train_std\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train, y_train_seq = create_sequences(y_train_norm.values, lookback)\n",
    "        \n",
    "        if len(X_train) < 50:\n",
    "            return np.full(len(y_test), y_train.mean()), None\n",
    "        \n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.Sequential([\n",
    "            layers.LSTM(50, activation='relu', return_sequences=True, input_shape=(lookback, 1)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.LSTM(50, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
    "        \n",
    "        # Forecast\n",
    "        last_sequence = y_train_norm.values[-lookback:]\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(len(y_test)):\n",
    "            X_pred = last_sequence.reshape((1, lookback, 1))\n",
    "            pred = model.predict(X_pred, verbose=0)[0, 0]\n",
    "            predictions.append(pred)\n",
    "            last_sequence = np.append(last_sequence[1:], pred)\n",
    "        \n",
    "        # Denormalize\n",
    "        predictions = np.array(predictions) * train_std + train_mean\n",
    "        return predictions, model\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "\n",
    "def train_gru(y_train, y_test, lookback=30):\n",
    "    \"\"\"GRU - Gated Recurrent Unit\"\"\"\n",
    "    if not KERAS_AVAILABLE:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "    \n",
    "    try:\n",
    "        train_mean, train_std = y_train.mean(), y_train.std()\n",
    "        y_train_norm = (y_train - train_mean) / train_std\n",
    "        \n",
    "        X_train, y_train_seq = create_sequences(y_train_norm.values, lookback)\n",
    "        \n",
    "        if len(X_train) < 50:\n",
    "            return np.full(len(y_test), y_train.mean()), None\n",
    "        \n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.GRU(50, activation='relu', return_sequences=True, input_shape=(lookback, 1)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.GRU(50, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
    "        \n",
    "        last_sequence = y_train_norm.values[-lookback:]\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(len(y_test)):\n",
    "            X_pred = last_sequence.reshape((1, lookback, 1))\n",
    "            pred = model.predict(X_pred, verbose=0)[0, 0]\n",
    "            predictions.append(pred)\n",
    "            last_sequence = np.append(last_sequence[1:], pred)\n",
    "        \n",
    "        predictions = np.array(predictions) * train_std + train_mean\n",
    "        return predictions, model\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "\n",
    "def train_cnn_lstm(y_train, y_test, lookback=30):\n",
    "    \"\"\"CNN-LSTM Hybrid\"\"\"\n",
    "    if not KERAS_AVAILABLE:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "    \n",
    "    try:\n",
    "        train_mean, train_std = y_train.mean(), y_train.std()\n",
    "        y_train_norm = (y_train - train_mean) / train_std\n",
    "        \n",
    "        X_train, y_train_seq = create_sequences(y_train_norm.values, lookback)\n",
    "        \n",
    "        if len(X_train) < 50:\n",
    "            return np.full(len(y_test), y_train.mean()), None\n",
    "        \n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(lookback, 1)),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.LSTM(50, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
    "        \n",
    "        last_sequence = y_train_norm.values[-lookback:]\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(len(y_test)):\n",
    "            X_pred = last_sequence.reshape((1, lookback, 1))\n",
    "            pred = model.predict(X_pred, verbose=0)[0, 0]\n",
    "            predictions.append(pred)\n",
    "            last_sequence = np.append(last_sequence[1:], pred)\n",
    "        \n",
    "        predictions = np.array(predictions) * train_std + train_mean\n",
    "        return predictions, model\n",
    "    except:\n",
    "        return np.full(len(y_test), y_train.mean()), None\n",
    "\n",
    "# --- 5. COMPREHENSIVE MODEL EVALUATION ---\n",
    "def evaluate_all_models(df_city, city_name):\n",
    "    \"\"\"Train and evaluate all 9 models\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"City: {city_name}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in df_city.columns \n",
    "                   if col not in ['Date', 'City', 'median'] and not df_city[col].isna().all()]\n",
    "    \n",
    "    df_clean = df_city.dropna(subset=feature_cols + ['median'])\n",
    "    \n",
    "    if len(df_clean) < 200:\n",
    "        print(f\"Insufficient data for {city_name}\")\n",
    "        return None\n",
    "    \n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean['median']\n",
    "    dates = df_clean['Date']\n",
    "    \n",
    "    # CV splits\n",
    "    cv_splits = time_series_cv_split(df_clean, n_splits=5, test_size=30)\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {\n",
    "        # Statistical\n",
    "        'ARIMA': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Statistical'},\n",
    "        'SARIMA': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Statistical'},\n",
    "        'Exp Smoothing': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Statistical'},\n",
    "        \n",
    "        # Machine Learning\n",
    "        'Random Forest': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Machine Learning'},\n",
    "        'Gradient Boosting': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Machine Learning'},\n",
    "        'SVR': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Machine Learning'},\n",
    "        \n",
    "        # Deep Learning\n",
    "        'LSTM': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Deep Learning'},\n",
    "        'GRU': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Deep Learning'},\n",
    "        'CNN-LSTM': {'mae': [], 'rmse': [], 'mape': [], 'family': 'Deep Learning'}\n",
    "    }\n",
    "    \n",
    "    print(f\"Running 5-Fold Time Series Cross-Validation...\")\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        print(f\"  Fold {fold+1}/5...\", end=' ')\n",
    "        \n",
    "        X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "        y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "        \n",
    "        # Statistical Models\n",
    "        pred_arima = train_arima(y_train, y_test)\n",
    "        pred_sarima = train_sarima(y_train, y_test)\n",
    "        pred_exp = train_exponential_smoothing(y_train, y_test)\n",
    "        \n",
    "        # ML Models\n",
    "        pred_rf, _ = train_random_forest(X_train, y_train, X_test)\n",
    "        pred_gb, _ = train_gradient_boosting(X_train, y_train, X_test)\n",
    "        pred_svr, _ = train_svr(X_train, y_train, X_test)\n",
    "        \n",
    "        # DL Models\n",
    "        pred_lstm, _ = train_lstm(y_train, y_test)\n",
    "        pred_gru, _ = train_gru(y_train, y_test)\n",
    "        pred_cnn_lstm, _ = train_cnn_lstm(y_train, y_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        predictions = {\n",
    "            'ARIMA': pred_arima,\n",
    "            'SARIMA': pred_sarima,\n",
    "            'Exp Smoothing': pred_exp,\n",
    "            'Random Forest': pred_rf,\n",
    "            'Gradient Boosting': pred_gb,\n",
    "            'SVR': pred_svr,\n",
    "            'LSTM': pred_lstm,\n",
    "            'GRU': pred_gru,\n",
    "            'CNN-LSTM': pred_cnn_lstm\n",
    "        }\n",
    "        \n",
    "        for model_name, pred in predictions.items():\n",
    "            mae = mean_absolute_error(y_test, pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "            mape = mean_absolute_percentage_error(y_test, pred)\n",
    "            \n",
    "            all_results[model_name]['mae'].append(mae)\n",
    "            all_results[model_name]['rmse'].append(rmse)\n",
    "            all_results[model_name]['mape'].append(mape)\n",
    "        \n",
    "        print(\"✓\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    summary = {}\n",
    "    for model_name in all_results:\n",
    "        summary[model_name] = {\n",
    "            'mae': np.mean(all_results[model_name]['mae']),\n",
    "            'rmse': np.mean(all_results[model_name]['rmse']),\n",
    "            'mape': np.mean(all_results[model_name]['mape']),\n",
    "            'mae_std': np.std(all_results[model_name]['mae']),\n",
    "            'family': all_results[model_name]['family']\n",
    "        }\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = min(summary, key=lambda x: summary[x]['mae'])\n",
    "    \n",
    "    print(f\"\\n{'Model':<20} {'Family':<18} {'MAE':<12} {'RMSE':<12} {'MAPE'}\")\n",
    "    print('-'*80)\n",
    "    \n",
    "    # Sort by family for better presentation\n",
    "    for family in ['Statistical', 'Machine Learning', 'Deep Learning']:\n",
    "        for model_name, metrics in sorted(summary.items(), key=lambda x: x[1]['mae']):\n",
    "            if metrics['family'] == family:\n",
    "                marker = \" ⭐\" if model_name == best_model_name else \"\"\n",
    "                print(f\"{model_name:<20} {family:<18} {metrics['mae']:>6.2f} ± {metrics['mae_std']:<3.2f} \"\n",
    "                      f\"{metrics['rmse']:>7.2f}     {metrics['mape']:>6.1%}{marker}\")\n",
    "    \n",
    "    # Retrain best model on full data\n",
    "    print(f\"\\nBest Model: {best_model_name} (MAE: {summary[best_model_name]['mae']:.2f})\")\n",
    "    print(\"Retraining on full dataset...\")\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        _, best_model = train_random_forest(X, y, X)\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        _, best_model = train_gradient_boosting(X, y, X)\n",
    "    elif best_model_name == 'SVR':\n",
    "        _, best_model = train_svr(X, y, X)\n",
    "    elif best_model_name == 'LSTM':\n",
    "        _, best_model = train_lstm(y, y)\n",
    "    elif best_model_name == 'GRU':\n",
    "        _, best_model = train_gru(y, y)\n",
    "    elif best_model_name == 'CNN-LSTM':\n",
    "        _, best_model = train_cnn_lstm(y, y)\n",
    "    else:\n",
    "        best_model = None\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'model_name': best_model_name,\n",
    "        'feature_cols': feature_cols,\n",
    "        'all_results': summary,\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'dates': dates,\n",
    "        'df_city': df_city\n",
    "    }\n",
    "\n",
    "# --- 6. GENERATE FORECASTS ---\n",
    "def generate_forecast(model_info, forecast_dates):\n",
    "    \"\"\"Generate forecasts for January 2026\"\"\"\n",
    "    \n",
    "    if model_info is None:\n",
    "        return None\n",
    "    \n",
    "    model = model_info['model']\n",
    "    model_name = model_info['model_name']\n",
    "    df_city = model_info['df_city']\n",
    "    \n",
    "    # For statistical/DL models, use different approach\n",
    "    if model_name in ['ARIMA', 'SARIMA', 'Exp Smoothing', 'LSTM', 'GRU', 'CNN-LSTM']:\n",
    "        y_series = df_city['median'].dropna()\n",
    "        \n",
    "        if model_name == 'ARIMA':\n",
    "            fitted = ARIMA(y_series, order=(5, 1, 2)).fit()\n",
    "            forecast_values = fitted.forecast(steps=31)\n",
    "        elif model_name == 'SARIMA':\n",
    "            fitted = SARIMAX(y_series, order=(2, 1, 2), seasonal_order=(1, 1, 1, 7)).fit(disp=False)\n",
    "            forecast_values = fitted.forecast(steps=31)\n",
    "        elif model_name == 'Exp Smoothing':\n",
    "            fitted = ExponentialSmoothing(y_series, seasonal_periods=7, trend='add', seasonal='add').fit()\n",
    "            forecast_values = fitted.forecast(steps=31)\n",
    "        else:  # DL models\n",
    "            lookback = 30\n",
    "            train_mean, train_std = y_series.mean(), y_series.std()\n",
    "            y_norm = (y_series - train_mean) / train_std\n",
    "            \n",
    "            last_sequence = y_norm.values[-lookback:]\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(31):\n",
    "                X_pred = last_sequence.reshape((1, lookback, 1))\n",
    "                pred = model.predict(X_pred, verbose=0)[0, 0]\n",
    "                predictions.append(pred)\n",
    "                last_sequence = np.append(last_sequence[1:], pred)\n",
    "            \n",
    "            forecast_values = np.array(predictions) * train_std + train_mean\n",
    "        \n",
    "        residuals = y_series.diff().dropna()\n",
    "        residual_std = residuals.std()\n",
    "        \n",
    "    else:  # ML models\n",
    "        feature_cols = model_info['feature_cols']\n",
    "        forecast_df = df_city.copy()\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for forecast_date in pd.date_range(forecast_dates[0], forecast_dates[1], freq='D'):\n",
    "            temp_row = pd.DataFrame({'Date': [forecast_date], 'City': [df_city['City'].iloc[0]]})\n",
    "            \n",
    "            temp_row['year'] = forecast_date.year\n",
    "            temp_row['month'] = forecast_date.month\n",
    "            temp_row['day'] = forecast_date.day\n",
    "            temp_row['dayofweek'] = forecast_date.dayofweek\n",
    "            temp_row['dayofyear'] = forecast_date.dayofyear\n",
    "            temp_row['quarter'] = forecast_date.quarter\n",
    "            temp_row['weekofyear'] = forecast_date.isocalendar().week\n",
    "            temp_row['month_sin'] = np.sin(2 * np.pi * forecast_date.month / 12)\n",
    "            temp_row['month_cos'] = np.cos(2 * np.pi * forecast_date.month / 12)\n",
    "            temp_row['day_sin'] = np.sin(2 * np.pi * forecast_date.dayofweek / 7)\n",
    "            temp_row['day_cos'] = np.cos(2 * np.pi * forecast_date.dayofweek / 7)\n",
    "            \n",
    "            for lag in [1, 2, 3, 7, 14, 30]:\n",
    "                if len(forecast_df) >= lag:\n",
    "                    temp_row[f'lag_{lag}'] = forecast_df['median'].iloc[-lag]\n",
    "                else:\n",
    "                    temp_row[f'lag_{lag}'] = forecast_df['median'].mean()\n",
    "            \n",
    "            for window in [7, 14, 30]:\n",
    "                if len(forecast_df) >= window:\n",
    "                    temp_row[f'rolling_mean_{window}'] = forecast_df['median'].iloc[-window:].mean()\n",
    "                    temp_row[f'rolling_std_{window}'] = forecast_df['median'].iloc[-window:].std()\n",
    "                    temp_row[f'rolling_min_{window}'] = forecast_df['median'].iloc[-window:].min()\n",
    "                    temp_row[f'rolling_max_{window}'] = forecast_df['median'].iloc[-window:].max()\n",
    "                else:\n",
    "                    temp_row[f'rolling_mean_{window}'] = forecast_df['median'].mean()\n",
    "                    temp_row[f'rolling_std_{window}'] = forecast_df['median'].std()\n",
    "                    temp_row[f'rolling_min_{window}'] = forecast_df['median'].min()\n",
    "                    temp_row[f'rolling_max_{window}'] = forecast_df['median'].max()\n",
    "            \n",
    "            temp_row['ewm_7'] = forecast_df['median'].iloc[-7:].mean() if len(forecast_df) >= 7 else forecast_df['median'].mean()\n",
    "            temp_row['ewm_30'] = forecast_df['median'].iloc[-30:].mean() if len(forecast_df) >= 30 else forecast_df['median'].mean()\n",
    "            \n",
    "            X_pred = temp_row[feature_cols]\n",
    "            \n",
    "            if model_name == 'SVR':\n",
    "                pred_value = model[0].predict(model[1].transform(X_pred))[0]\n",
    "            else:\n",
    "                pred_value = model.predict(X_pred)[0]\n",
    "            \n",
    "            predictions.append(pred_value)\n",
    "            \n",
    "            temp_row['median'] = pred_value\n",
    "            forecast_df = pd.concat([forecast_df, temp_row], ignore_index=True)\n",
    "        \n",
    "        forecast_values = np.array(predictions)\n",
    "        residuals = model_info['y'] - model.predict(model_info['X'])\n",
    "        residual_std = residuals.std()\n",
    "    \n",
    "    prediction_dates = pd.date_range(forecast_dates[0], forecast_dates[1], freq='D')\n",
    "    \n",
    "    forecast_result = pd.DataFrame({\n",
    "        'Date': prediction_dates,\n",
    "        'Prediction': forecast_values,\n",
    "        'Lower_95': forecast_values - 1.96 * residual_std,\n",
    "        'Upper_95': forecast_values + 1.96 * residual_std,\n",
    "        'Lower_80': forecast_values - 1.28 * residual_std,\n",
    "        'Upper_80': forecast_values + 1.28 * residual_std\n",
    "    })\n",
    "    \n",
    "    return forecast_result\n",
    "\n",
    "# --- 7. MAIN EXECUTION ---\n",
    "cities = df['City'].unique()\n",
    "all_forecasts = {}\n",
    "all_model_info = {}\n",
    "\n",
    "for city in cities:\n",
    "    df_city = create_features(df, city)\n",
    "    model_info = evaluate_all_models(df_city, city)\n",
    "    \n",
    "    if model_info is not None:\n",
    "        forecast = generate_forecast(model_info, [FORECAST_START, FORECAST_END])\n",
    "        forecast['City'] = city\n",
    "        \n",
    "        all_forecasts[city] = forecast\n",
    "        all_model_info[city] = model_info\n",
    "\n",
    "# --- 8. SAVE RESULTS ---\n",
    "if all_forecasts:\n",
    "    final_predictions = pd.concat(all_forecasts.values(), ignore_index=True)\n",
    "    final_predictions.to_csv(os.path.join(PREDICTIONS_DIR, 'pm25_forecast_2026_01.csv'), index=False)\n",
    "    final_predictions.to_parquet(os.path.join(PREDICTIONS_DIR, 'pm25_forecast_2026_01.parquet'), index=False)\n",
    "    \n",
    "    # Save model comparison\n",
    "    import json\n",
    "    model_comparison = {}\n",
    "    for city, info in all_model_info.items():\n",
    "        model_comparison[city] = {\n",
    "            'best_model': info['model_name'],\n",
    "            'all_scores': {k: {'mae': v['mae'], 'rmse': v['rmse'], 'mape': v['mape'], 'family': v['family']} \n",
    "                          for k, v in info['all_results'].items()}\n",
    "        }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'model_comparison.json'), 'w') as f:\n",
    "        json.dump(model_comparison, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORECASTING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"✓ Predictions saved: {PREDICTIONS_DIR}\")\n",
    "    print(f\"✓ Model comparison saved: {OUTPUT_DIR}/model_comparison.json\")\n",
    "    print(\"\\nBest Models by City:\")\n",
    "    for city, info in all_model_info.items():\n",
    "        print(f\"  {city:15} → {info['model_name']:20} (MAE: {info['all_results'][info['model_name']]['mae']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b162d8",
   "metadata": {},
   "source": [
    "<h2>96 minutes to train all the models of 6 cities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
